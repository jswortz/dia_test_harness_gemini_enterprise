import logging
import re
import os

# Configure connection pool size for Vertex AI SDK BEFORE importing
# This sets urllib3's default maxsize for ALL PoolManagers
os.environ.setdefault('URLLIB3_DEFAULT_POOL_SIZE', '200')

import vertexai
from vertexai.generative_models import GenerativeModel
import urllib3

# Set urllib3 default pool size to handle high concurrency
# This affects all HTTPConnectionPool instances created after this point
urllib3.poolmanager.PoolManager.DEFAULT_POOLSIZE = 200

class SQLComparator:
    def compare(self, generated: str, expected: str) -> bool:
        """
        Compares two SQL strings for exact match, ignoring leading/trailing whitespace
        and collapsing internal whitespace to single spaces.
        """
        if not generated or not expected:
            return False
            
        def normalize(s):
            # Replace multiple whitespace with single space and strip
            return re.sub(r'\s+', ' ', s).strip()
            
        return normalize(generated) == normalize(expected)

class JudgementModel:
    def __init__(self, project_id: str, location: str, model_name: str = "gemini-2.5-pro"):
        """
        Initialize JudgementModel.

        Args:
            project_id: Google Cloud project ID
            location: Vertex AI location
            model_name: Model to use for judgement (default: gemini-2.5-pro)

        Note:
            Connection pool is configured globally at module level (200 connections)
            to handle high concurrency from parallel workers.
        """
        vertexai.init(project=project_id, location=location)
        self.model = GenerativeModel(model_name)

    def explain_difference(self, question: str, generated_sql: str, expected_sql: str,
                          thoughts: str = "", agent_response: str = "", schema_info: str = "") -> str:
        """
        Uses a generative model to explain the difference between generated and expected SQL.
        Incorporates best practices for SQL semantic equivalence evaluation:
        - Miniature & Mull prompting: Consider execution on sample data
        - Schema-aware evaluation: Uses relevant table/column context
        - Semantic equivalence patterns: Recognizes common SQL transformations
        - Multi-aspect rubrics: Evaluates logic, performance, and correctness

        Args:
            question: Natural language question
            generated_sql: SQL generated by the agent
            expected_sql: Golden/expected SQL
            thoughts: Agent's internal reasoning
            agent_response: Agent's natural language response
            schema_info: Relevant schema description (recommended for accuracy)
        """
        # Extract table names from queries for schema filtering
        schema_context = ""
        if schema_info:
            schema_context = f"""
RELEVANT SCHEMA INFORMATION:
{schema_info}
"""

        prompt = f"""You are an expert SQL semantic equivalence judge with deep knowledge of database systems and query optimization.

Your task is to determine if two SQL queries are semantically equivalent (would produce the same result set) even if syntactically different.

=== EVALUATION FRAMEWORK ===

Use the "Miniature & Mull" technique:
1. Consider how each query would execute on a small, representative database instance
2. Think through the logical operations: filtering, joining, aggregation, ordering
3. Look for counterexamples where results would differ
4. If no counterexample exists, queries are EQUIVALENT

=== SEMANTIC EQUIVALENCE PATTERNS (Common transformations that preserve semantics) ===

1. JOIN VARIATIONS:
   - INNER JOIN vs WHERE clause with table list
   - Explicit vs implicit joins
   - Different join order (when results are same)

2. AGGREGATION EQUIVALENCES:
   - DISTINCT vs GROUP BY (for unique values)
   - COUNT(DISTINCT col) vs COUNT(*) with GROUP BY
   - Different GROUP BY orderings

3. FILTERING EQUIVALENCES:
   - OR conditions vs UNION
   - IN clause vs multiple OR
   - Subquery vs JOIN for filtering

4. SYNTAX VARIATIONS:
   - Table/column aliases vs full names
   - Different whitespace/formatting
   - Explicit vs implicit table prefixes
   - String case differences in identifiers (if case-insensitive)

5. DATE/TIME VARIATIONS:
   - Different date formats producing same range
   - BETWEEN vs >= AND <=
   - Different date functions yielding same result

6. CALCULATION EQUIVALENCES:
   - Mathematical expressions reordered
   - Different rounding approaches (if tolerance acceptable)
   - Percentage calculations with same logic

=== EVALUATION RUBRIC ===

Assign scores for:
- **Logical Equivalence** (0-10): Do queries have same logical structure?
- **Result Set Match** (0-10): Would they return identical results?
- **Performance Similarity** (0-5): Similar execution characteristics?

Score >= 25/25 → EQUIVALENT
Score < 25/25 → DIFFERENT

=== COMMON ERROR PATTERNS (Usually DIFFERENT) ===

- Missing/extra JOIN conditions
- Incorrect aggregation grouping
- Wrong filtering logic (AND vs OR)
- Date range errors
- Missing DISTINCT when needed
- Incorrect column references
- Wrong table references
- Calculation errors in formulas

=== INPUTS ===

Natural Language Question:
"{question}"

Expected SQL (Golden Standard):
```sql
{expected_sql}
```

Generated SQL (Agent Output):
```sql
{generated_sql}
```

Agent's Internal Thoughts:
{thoughts if thoughts else "(No thoughts provided)"}

Agent's Response:
{agent_response if agent_response else "(No response provided)"}
{schema_context}

=== YOUR ANALYSIS ===

Please provide:

1. **Key Differences**: List the syntactic/structural differences
2. **Semantic Analysis**: Apply "Miniature & Mull" - walk through query execution
3. **Equivalence Patterns**: Which patterns (if any) apply?
4. **Scoring**:
   - Logical Equivalence: X/10
   - Result Set Match: X/10
   - Performance Similarity: X/5
   - **Total: X/25**
5. **Counterexample**: If DIFFERENT, provide an example where results diverge
6. **Final Judgment**: Either "EQUIVALENT" or "DIFFERENT"

Format your response concisely but include all sections.
        """
        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            logging.error(f"Error generating judgement: {e}")
            return f"Error resolving judgment: {str(e)}"
