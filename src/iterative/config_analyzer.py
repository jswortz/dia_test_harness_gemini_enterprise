"""
Configuration Field Analyzer using Gemini 2.0 Flash.

This module provides intelligent analysis of which configuration fields should be
improved based on test failures. It uses Gemini to provide structured recommendations
for modifying agent configurations.
"""

import json
import logging
from typing import Any, Dict, List, Optional

import vertexai
from vertexai.generative_models import GenerativeModel

logger = logging.getLogger(__name__)


class ConfigFieldAnalyzer:
    """
    Analyzes configuration fields using Gemini 2.0 Flash to determine which fields
    should be modified based on test failures.

    Analyzes the following configuration fields:
    - nl2sql_prompt: Natural language to SQL conversion prompt
    - schema_description: Database schema description
    - nl2sql_examples: Example question-SQL pairs
    - nl2py_prompt: Natural language to Python conversion prompt
    """

    ANALYZABLE_FIELDS = [
        "nl2sql_prompt",
        "schema_description",
        "nl2sql_examples",
        "nl2py_prompt"
    ]

    def __init__(
        self,
        project_id: str,
        location: str = "us-central1",
        model_name: str = "gemini-3-pro-preview"
    ):
        """
        Initialize the ConfigFieldAnalyzer.

        Args:
            project_id: Google Cloud project ID
            location: Google Cloud location for Vertex AI
            model_name: Gemini model to use for analysis
        """
        self.project_id = project_id
        self.location = location
        self.model_name = model_name

        # Initialize Vertex AI
        vertexai.init(project=project_id, location=location)
        self.model = GenerativeModel(model_name)

        logger.info(
            f"Initialized ConfigFieldAnalyzer with model={model_name}, "
            f"project={project_id}, location={location}"
        )

    def analyze_config_improvements(
        self,
        failures: List[Dict[str, Any]],
        current_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Analyze test failures and recommend configuration field improvements.

        Args:
            failures: List of test failure dictionaries, each containing:
                - question: The test question
                - expected_sql: Expected SQL query
                - generated_sql: SQL generated by agent
                - error: Error message (if applicable)
                - comparison_result: Result from SQL comparison
            current_config: Current agent configuration dictionary

        Returns:
            Dictionary with field recommendations in format:
            {
                "field_recommendations": {
                    "field_name": {
                        "should_modify": bool,
                        "rationale": str,
                        "priority": int (1-5),
                        "suggested_value": str
                    },
                    ...
                }
            }
        """
        logger.info(f"Analyzing {len(failures)} failures for config improvements")

        # Build the analysis prompt
        prompt = self._build_analysis_prompt(failures, current_config)

        try:
            # Generate analysis using Gemini
            response = self.model.generate_content(prompt)
            response_text = response.text

            logger.debug(f"Gemini response: {response_text}")

            # Parse the JSON response
            recommendations = self._parse_recommendations(response_text)

            logger.info(
                f"Successfully analyzed config fields. "
                f"Recommendations generated for {len(recommendations.get('field_recommendations', {}))} fields"
            )

            return recommendations

        except Exception as e:
            logger.error(f"Error during config analysis: {e}", exc_info=True)
            return self._get_fallback_recommendations()

    def _build_analysis_prompt(
        self,
        failures: List[Dict[str, Any]],
        current_config: Dict[str, Any]
    ) -> str:
        """
        Build a comprehensive prompt for Gemini to analyze configuration improvements.

        Args:
            failures: List of test failures
            current_config: Current configuration

        Returns:
            Formatted prompt string
        """
        # Format failures for the prompt
        failures_summary = self._format_failures(failures)

        # Format current config values
        config_summary = self._format_current_config(current_config)

        prompt = f"""You are an expert in Data Insights Agent configuration for BigQuery natural language to SQL systems.

Your task is to analyze test failures and recommend which configuration fields should be modified to improve agent performance.

## Available Configuration Fields

1. **nl2sql_prompt**: The system prompt that guides natural language to SQL conversion. This is the primary instruction for SQL generation.
   - Modify when: SQL structure is incorrect, agent misunderstands query intent, or needs better guidance
   - Priority: HIGH - Most impactful field

2. **schema_description**: Detailed description of the database schema, tables, columns, and relationships.
   - Modify when: Agent uses wrong tables/columns, misunderstands schema relationships
   - Priority: HIGH - Critical for correct SQL generation

3. **nl2sql_examples**: Few-shot examples showing questionâ†’SQL pairs for pattern learning.
   - Modify when: Agent needs examples of specific query patterns it's failing on
   - Priority: MEDIUM - Helps with specific patterns

4. **nl2py_prompt**: System prompt for natural language to Python conversion (for data manipulation).
   - Modify when: Failures involve Python-based data processing or transformations
   - Priority: LOW - Only if Python processing is involved

## Current Configuration

{config_summary}

## Test Failures to Analyze

{failures_summary}

## Your Task

Analyze these failures and provide recommendations for EACH of the 4 configuration fields listed above.

For each field, determine:
1. **should_modify** (bool): Whether this field should be changed
2. **rationale** (str): Clear explanation of why this field should/shouldn't be modified based on the failures
3. **priority** (int 1-5): How important this modification is (5=critical, 1=nice-to-have)
4. **suggested_value** (str): If should_modify=True, provide a concrete suggestion for the new value

## Output Format

Respond with ONLY a valid JSON object in this exact format (no markdown, no code blocks):

{{
  "field_recommendations": {{
    "nl2sql_prompt": {{
      "should_modify": true/false,
      "rationale": "explanation here",
      "priority": 1-5,
      "suggested_value": "new value if should_modify=true, otherwise empty string"
    }},
    "schema_description": {{
      "should_modify": true/false,
      "rationale": "explanation here",
      "priority": 1-5,
      "suggested_value": "new value if should_modify=true, otherwise empty string"
    }},
    "nl2sql_examples": {{
      "should_modify": true/false,
      "rationale": "explanation here",
      "priority": 1-5,
      "suggested_value": "new value if should_modify=true, otherwise empty string"
    }},
    "nl2py_prompt": {{
      "should_modify": true/false,
      "rationale": "explanation here",
      "priority": 1-5,
      "suggested_value": "new value if should_modify=true, otherwise empty string"
    }}
  }}
}}

IMPORTANT:
- Provide recommendations for ALL 4 fields
- Be specific in rationale - reference actual failures
- Suggested values should be actionable and concrete
- Only output the JSON, nothing else
"""
        return prompt

    def _format_failures(self, failures: List[Dict[str, Any]]) -> str:
        """Format test failures for inclusion in the prompt."""
        if not failures:
            return "No failures to analyze."

        formatted = []
        for i, failure in enumerate(failures[:10], 1):  # Limit to first 10 for prompt size
            question = failure.get("question", "N/A")
            expected = failure.get("expected_sql", "N/A")
            generated = failure.get("generated_sql", "N/A")
            error = failure.get("error", "")
            comparison = failure.get("comparison_result", {})

            formatted.append(f"""
### Failure {i}
- Question: {question}
- Expected SQL: {expected}
- Generated SQL: {generated}
- Error: {error if error else "None"}
- Comparison Result: {json.dumps(comparison, indent=2) if comparison else "N/A"}
""")

        if len(failures) > 10:
            formatted.append(f"\n... and {len(failures) - 10} more failures")

        return "\n".join(formatted)

    def _format_current_config(self, config: Dict[str, Any]) -> str:
        """Format current configuration for inclusion in the prompt."""
        formatted = []

        for field in self.ANALYZABLE_FIELDS:
            value = config.get(field, "Not set")

            # Truncate very long values
            if isinstance(value, str) and len(value) > 500:
                value = value[:500] + "... (truncated)"
            elif isinstance(value, list):
                value = json.dumps(value, indent=2)
                if len(value) > 500:
                    value = value[:500] + "... (truncated)"

            formatted.append(f"**{field}**: {value}")

        return "\n".join(formatted)

    def _parse_recommendations(self, response_text: str) -> Dict[str, Any]:
        """
        Parse Gemini's response to extract structured recommendations.

        Handles various response formats and JSON parsing errors gracefully.

        Args:
            response_text: Raw text response from Gemini

        Returns:
            Parsed recommendations dictionary
        """
        try:
            # Try to extract JSON from markdown code blocks
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()

            # Parse JSON
            recommendations = json.loads(response_text)

            # Validate structure
            if "field_recommendations" not in recommendations:
                logger.warning("Response missing 'field_recommendations' key")
                return self._get_fallback_recommendations()

            # Validate each field recommendation
            for field in self.ANALYZABLE_FIELDS:
                if field not in recommendations["field_recommendations"]:
                    logger.warning(f"Missing recommendation for field: {field}")
                    recommendations["field_recommendations"][field] = {
                        "should_modify": False,
                        "rationale": "No recommendation generated",
                        "priority": 1,
                        "suggested_value": ""
                    }
                else:
                    # Ensure all required keys exist
                    rec = recommendations["field_recommendations"][field]
                    if "should_modify" not in rec:
                        rec["should_modify"] = False
                    if "rationale" not in rec:
                        rec["rationale"] = "No rationale provided"
                    if "priority" not in rec:
                        rec["priority"] = 1
                    if "suggested_value" not in rec:
                        rec["suggested_value"] = ""

            return recommendations

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from Gemini response: {e}")
            logger.debug(f"Response text: {response_text}")
            return self._get_fallback_recommendations()
        except Exception as e:
            logger.error(f"Error parsing recommendations: {e}", exc_info=True)
            return self._get_fallback_recommendations()

    def _get_fallback_recommendations(self) -> Dict[str, Any]:
        """
        Return fallback recommendations when parsing fails.

        Returns conservative recommendations that suggest not modifying anything.
        """
        return {
            "field_recommendations": {
                field: {
                    "should_modify": False,
                    "rationale": "Analysis failed - no recommendation available",
                    "priority": 1,
                    "suggested_value": ""
                }
                for field in self.ANALYZABLE_FIELDS
            }
        }
