"""
Configuration Field Analyzer using Gemini 2.0 Flash.

This module provides intelligent analysis of which configuration fields should be
improved based on test failures. It uses Gemini to provide structured recommendations
for modifying agent configurations.
"""

import json
import logging
from typing import Any, Dict, List, Optional

import vertexai
from vertexai.generative_models import GenerativeModel

logger = logging.getLogger(__name__)


class ConfigFieldAnalyzer:
    """
    Analyzes configuration fields using Gemini 2.0 Flash to determine which fields
    should be modified based on test failures.

    Analyzes the following configuration fields:
    - nl2sql_prompt: Natural language to SQL conversion prompt
    - tool_description: Agent-level description for routing and scoping
    - schema_description: Database schema description
    - nl2sql_examples: Example question-SQL pairs
    - nl2py_prompt: Natural language to Python conversion prompt
    """

    ANALYZABLE_FIELDS = [
        "nl2sql_prompt",
        "tool_description",
        "schema_description",
        "nl2sql_examples",
        "nl2py_prompt"
    ]

    def __init__(
        self,
        project_id: str,
        location: str = "us-central1",
        model_name: str = "gemini-3-pro-preview"
    ):
        """
        Initialize the ConfigFieldAnalyzer.

        Args:
            project_id: Google Cloud project ID
            location: Google Cloud location for Vertex AI
            model_name: Gemini model to use for analysis
        """
        self.project_id = project_id
        self.location = location
        self.model_name = model_name

        # Initialize Vertex AI
        vertexai.init(project=project_id, location=location)
        self.model = GenerativeModel(model_name)

        logger.info(
            f"Initialized ConfigFieldAnalyzer with model={model_name}, "
            f"project={project_id}, location={location}"
        )

    def analyze_config_improvements(
        self,
        failures: List[Dict[str, Any]],
        current_config: Dict[str, Any],
        successes: Optional[List[Dict[str, Any]]] = None,
        previous_metrics: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Analyze test failures and recommend configuration field improvements.

        **CRITICAL: This method must ONLY receive TRAINING data to avoid data leakage.**
        All failures and successes MUST be from the training set. Test set data
        must NEVER be passed to this method, as it would leak information from
        the held-out test set into the optimization process.

        Args:
            failures: List of test failure dictionaries FROM TRAINING SET ONLY, each containing:
                - question: The test question
                - expected_sql: Expected SQL query
                - generated_sql: SQL generated by agent
                - error: Error message (if applicable)
                - comparison_result: Result from SQL comparison
            current_config: Current agent configuration dictionary
            successes: Optional list of successful test cases FROM TRAINING SET ONLY for pattern insights
            previous_metrics: Optional previous iteration metrics for trajectory analysis

        Returns:
            Dictionary with field recommendations in format:
            {
                "field_recommendations": {
                    "field_name": {
                        "should_modify": bool,
                        "rationale": str,
                        "priority": int (1-5),
                        "suggested_value": str
                    },
                    ...
                }
            }
        """
        logger.info(f"Analyzing {len(failures)} failures for config improvements")
        if successes:
            logger.info(f"Including {len(successes)} successful cases for pattern insights")
        if previous_metrics:
            logger.info(f"Including previous iteration metrics for trajectory analysis")

        # Build the analysis prompt with trajectory context
        prompt = self._build_analysis_prompt(failures, current_config, successes, previous_metrics)

        try:
            # Generate analysis using Gemini
            response = self.model.generate_content(prompt)
            response_text = response.text

            logger.debug(f"Gemini response: {response_text}")

            # Parse the JSON response
            recommendations = self._parse_recommendations(response_text)

            logger.info(
                f"Successfully analyzed config fields. "
                f"Recommendations generated for {len(recommendations.get('field_recommendations', {}))} fields"
            )

            return recommendations

        except Exception as e:
            logger.error(f"Error during config analysis: {e}", exc_info=True)
            return self._get_fallback_recommendations()

    def _build_analysis_prompt(
        self,
        failures: List[Dict[str, Any]],
        current_config: Dict[str, Any],
        successes: Optional[List[Dict[str, Any]]] = None,
        previous_metrics: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Build a comprehensive prompt for Gemini to analyze configuration improvements.

        Args:
            failures: List of test failures
            current_config: Current configuration
            successes: Optional list of successful test cases
            previous_metrics: Optional previous iteration metrics for trajectory analysis

        Returns:
            Formatted prompt string
        """
        # Build trajectory context section if previous metrics exist
        trajectory_warning = ""
        if previous_metrics:
            # Extract accuracy from previous metrics (handle both dict and float formats)
            if isinstance(previous_metrics.get("accuracy"), dict):
                prev_accuracy = previous_metrics["accuracy"].get("mean", 0.0)
            else:
                prev_accuracy = previous_metrics.get("accuracy", 0.0)

            # Calculate current accuracy from failures and successes
            total = len(failures) + (len(successes) if successes else 0)
            current_correct = len(successes) if successes else 0
            current_accuracy = (current_correct / total * 100.0) if total > 0 else 0.0

            # Convert prev_accuracy to percentage if it's not already
            if prev_accuracy <= 1.0:
                prev_accuracy = prev_accuracy * 100.0

            # Check for regression
            if current_accuracy < prev_accuracy:
                decline = prev_accuracy - current_accuracy
                trajectory_warning = f"""
## ⚠️ PERFORMANCE REGRESSION DETECTED

**Previous iteration accuracy**: {prev_accuracy:.2f}%
**Current iteration accuracy**: {current_accuracy:.2f}%
**Decline**: {decline:.2f}% (performance got WORSE)

**Your recommendations should consider**:
- Which recent config changes may have caused this regression?
- Should we revert some changes instead of making new ones?
- Can we identify specific changes that hurt performance?
- How can we recover the previous performance level?

**CRITICAL**: Focus on understanding WHY performance degraded and suggesting fixes.
"""

        # Format failures for the prompt
        failures_summary = self._format_failures(failures)

        # Format successes for the prompt
        successes_summary = self._format_successes(successes) if successes else ""

        # Format current config values
        config_summary = self._format_current_config(current_config)

        prompt = f"""You are an expert in Data Insights Agent configuration for BigQuery natural language to SQL systems.

Your task is to analyze test failures and recommend which configuration fields should be modified to improve agent performance.

## Available Configuration Fields

1. **nl2sql_prompt**: The system prompt that guides natural language to SQL conversion. This is the primary instruction for SQL generation.
   - Modify when: SQL structure is incorrect, agent misunderstands query intent, or needs better guidance
   - Priority: HIGH - Most impactful field

2. **tool_description**: Description of when and how to use this agent (agent-level guidance for routing).
   - Modify when: Agent needs clearer scope definition or usage guidance
   - Priority: MEDIUM - Helps with agent selection and scoping

3. **schema_description**: Detailed description of the database schema, tables, columns, and relationships.
   - Modify when: Agent uses wrong tables/columns, misunderstands schema relationships
   - Priority: HIGH - Critical for correct SQL generation

4. **nl2sql_examples**: Few-shot examples showing question→SQL pairs for pattern learning.
   - Modify when: Agent needs examples of specific query patterns it's failing on
   - Priority: MEDIUM - Helps with specific patterns

5. **nl2py_prompt**: System prompt for natural language to Python conversion (for data manipulation).
   - Modify when: Failures involve Python-based data processing or transformations
   - Priority: LOW - Only if Python processing is involved

## Current Configuration

{config_summary}

{trajectory_warning}
## Training Set Failures to Analyze

{failures_summary}
{successes_summary}
## Your Task

Analyze these TRAINING SET failures and provide recommendations for EACH of the 4 configuration fields listed above.

**CRITICAL DATA LEAKAGE PREVENTION:**
- You are analyzing ONLY training set data (failures and successes)
- Do NOT make assumptions about held-out test set performance
- Focus improvements on patterns observed in the training data
- Use successful TRAINING cases to understand what patterns work well and should be preserved

For each field, determine:
1. **should_modify** (bool): Whether this field should be changed
2. **rationale** (str): Clear explanation of why this field should/shouldn't be modified based on the failures
3. **priority** (int 1-5): How important this modification is (5=critical, 1=nice-to-have)
4. **suggested_value** (str): If should_modify=True, provide a concrete suggestion for the new value

## Output Format

Respond with ONLY a valid JSON object in this exact format (no markdown, no code blocks):

{{
  "field_recommendations": {{
    "nl2sql_prompt": {{
      "should_modify": true/false,
      "rationale": "explanation here",
      "priority": 1-5,
      "suggested_value": "new value if should_modify=true, otherwise empty string"
    }},
    "tool_description": {{
      "should_modify": true/false,
      "rationale": "explanation here",
      "priority": 1-5,
      "suggested_value": "new value if should_modify=true, otherwise empty string"
    }},
    "schema_description": {{
      "should_modify": true/false,
      "rationale": "explanation here",
      "priority": 1-5,
      "suggested_value": "new value if should_modify=true, otherwise empty string"
    }},
    "nl2sql_examples": {{
      "should_modify": true/false,
      "rationale": "explanation here",
      "priority": 1-5,
      "suggested_value": "new value if should_modify=true, otherwise empty string"
    }},
    "nl2py_prompt": {{
      "should_modify": true/false,
      "rationale": "explanation here",
      "priority": 1-5,
      "suggested_value": "new value if should_modify=true, otherwise empty string"
    }}
  }}
}}

IMPORTANT:
- Provide recommendations for ALL 4 fields
- Be specific in rationale - reference actual failures
- Suggested values should be actionable and concrete
- Only output the JSON, nothing else
"""
        return prompt

    def _format_failures(self, failures: List[Dict[str, Any]]) -> str:
        """Format test failures for inclusion in the prompt."""
        if not failures:
            return "No failures to analyze."

        formatted = []
        for i, failure in enumerate(failures[:10], 1):  # Limit to first 10 for prompt size
            question = failure.get("question", "N/A")
            expected = failure.get("expected_sql", "N/A")
            generated = failure.get("generated_sql", "N/A")
            error = failure.get("error", "")
            comparison = failure.get("comparison_result", {})

            formatted.append(f"""
### Failure {i}
- Question: {question}
- Expected SQL: {expected}
- Generated SQL: {generated}
- Error: {error if error else "None"}
- Comparison Result: {json.dumps(comparison, indent=2) if comparison else "N/A"}
""")

        if len(failures) > 10:
            formatted.append(f"\n... and {len(failures) - 10} more failures")

        return "\n".join(formatted)

    def _format_successes(self, successes: Optional[List[Dict[str, Any]]]) -> str:
        """Format successful test cases for inclusion in the prompt."""
        if not successes:
            return ""

        formatted = []
        # Limit to first 5 successes to avoid prompt bloat
        for i, success in enumerate(successes[:5], 1):
            question = success.get("question", "N/A")
            expected = success.get("expected_sql", "N/A")
            generated = success.get("generated_sql", "N/A")

            formatted.append(f"""
### Success {i}
- Question: {question}
- Expected SQL: {expected}
- Generated SQL: {generated}
""")

        if len(successes) > 5:
            formatted.append(f"\n... and {len(successes) - 5} more successful cases")

        return f"""
## Successful TRAINING Cases (showing what works well)

{"".join(formatted)}

Use these successful TRAINING patterns to understand what the agent is doing correctly and preserve those capabilities.
**IMPORTANT:** These are TRAINING set successes only - do not assume test set behavior.
"""

    def _format_current_config(self, config: Dict[str, Any]) -> str:
        """Format current configuration for inclusion in the prompt."""
        formatted = []

        for field in self.ANALYZABLE_FIELDS:
            value = config.get(field, "Not set")

            # Truncate very long values
            if isinstance(value, str) and len(value) > 500:
                value = value[:500] + "... (truncated)"
            elif isinstance(value, list):
                value = json.dumps(value, indent=2)
                if len(value) > 500:
                    value = value[:500] + "... (truncated)"

            formatted.append(f"**{field}**: {value}")

        return "\n".join(formatted)

    def _parse_recommendations(self, response_text: str) -> Dict[str, Any]:
        """
        Parse Gemini's response to extract structured recommendations.

        Handles various response formats and JSON parsing errors gracefully.

        Args:
            response_text: Raw text response from Gemini

        Returns:
            Parsed recommendations dictionary
        """
        try:
            # Try to extract JSON from markdown code blocks
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()

            # Parse JSON
            recommendations = json.loads(response_text)

            # Validate structure
            if "field_recommendations" not in recommendations:
                logger.warning("Response missing 'field_recommendations' key")
                return self._get_fallback_recommendations()

            # Validate each field recommendation
            for field in self.ANALYZABLE_FIELDS:
                if field not in recommendations["field_recommendations"]:
                    logger.warning(f"Missing recommendation for field: {field}")
                    recommendations["field_recommendations"][field] = {
                        "should_modify": False,
                        "rationale": "No recommendation generated",
                        "priority": 1,
                        "suggested_value": ""
                    }
                else:
                    # Ensure all required keys exist
                    rec = recommendations["field_recommendations"][field]
                    if "should_modify" not in rec:
                        rec["should_modify"] = False
                    if "rationale" not in rec:
                        rec["rationale"] = "No rationale provided"
                    if "priority" not in rec:
                        rec["priority"] = 1
                    if "suggested_value" not in rec:
                        rec["suggested_value"] = ""

            return recommendations

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from Gemini response: {e}")
            logger.debug(f"Response text: {response_text}")
            return self._get_fallback_recommendations()
        except Exception as e:
            logger.error(f"Error parsing recommendations: {e}", exc_info=True)
            return self._get_fallback_recommendations()

    def _get_fallback_recommendations(self) -> Dict[str, Any]:
        """
        Return fallback recommendations when parsing fails.

        Returns conservative recommendations that suggest not modifying anything.
        """
        return {
            "field_recommendations": {
                field: {
                    "should_modify": False,
                    "rationale": "Analysis failed - no recommendation available",
                    "priority": 1,
                    "suggested_value": ""
                }
                for field in self.ANALYZABLE_FIELDS
            }
        }
